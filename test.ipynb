{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing of Requirements (only run this if you haven't installed the necessary libraries found in requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as  gpd\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "- convert all geojson files into 1 geodataframe \n",
    "- data cleaning\n",
    "- clip the inputs onto the map layer, similar to getting the intersection between the points and the map layer\n",
    "- return the output onto a file\n",
    "- write documentation in this notebook if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Paranaque Images and Saving a compiled GEOJSON file of the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the data of Paranque images\n",
    "paranaque_json_dir = 'data/paranaque'\n",
    "\n",
    "os.chdir(paranaque_json_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_list = []\n",
    "geojson_Count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith('.geojson'):\n",
    "        gdf = gpd.read_file(file)\n",
    "        geojson_Count += 1\n",
    "        \n",
    "        # filter out null json files\n",
    "        if '\"features\": []' not in gdf:\n",
    "            gdf = gpd.read_file(file)\n",
    "            gdf_list.append(gdf)\n",
    "            \n",
    "            \n",
    "        # gdf_list.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this if you want to view the compiled json\n",
    "\n",
    "# gdf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the compiled images into a GeoDataFrame in a GEOJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'output_geojson'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# merged_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "# output_filename = 'output_geojson/merged_paranaque.geojson'\n",
    "# merged_gdf.to_file(output_filename, driver='GeoJSON')\n",
    "# print(f\"Saved {output_filename}\")\n",
    "\n",
    "#the file is found in a folder that can be viewed in the data/paranaque folder as \"output_geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "def hash_image(file_path):\n",
    "    \"\"\"\n",
    "    Compute a hash for an image file.\n",
    "    \"\"\"\n",
    "    with Image.open(file_path) as img:\n",
    "        return imagehash.average_hash(img)\n",
    "\n",
    "def filter_similar_images(gdf_list):\n",
    "    \"\"\"\n",
    "    Filter out similar images based on their hash.\n",
    "    \"\"\"\n",
    "    filtered_gdf_list = []\n",
    "    hash_set = set()\n",
    "\n",
    "    for gdf in gdf_list:\n",
    "        file_path = gdf['thumb_2048_url']  # Replace 'your_image_column' with the column containing image file paths\n",
    "        file_hash = hash_image(file_path)\n",
    "\n",
    "        if file_hash not in hash_set:\n",
    "            hash_set.add(file_hash)\n",
    "            filtered_gdf_list.append(gdf)\n",
    "\n",
    "    return filtered_gdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "# from io import BytesIO\n",
    "\n",
    "# url145_2 = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9cxUlW_AayC--U1c1tXtigRWBRepZES4Saq31GZPPfI_1kASse71rEi1aHOXJg7PgemaXitnPVZj7-Gu3DCgPYpqEpwxl8xyqfFQExorMuMYfThwfTk1o7hkNvHVDmUNNzmOxxMihFXrV1DWZbUQ?stp=s2048x1152&ccb=10-5&oh=00_AfABtAA7BtX7l74UEQ9ZLtj1PaNJfoLMuund6A1JT3mVIQ&oe=65B3421E&_nc_sid=201bca\"\n",
    "# url145_x = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An_KYEqGfSXn0q4WRrqa5yKMBoBGH2ASguDbtT_RWVlbONyd3wa2GQn_D2n5o1hEzzh3HJjCCRpBnakmbUbOvOxrjKhCTQD0j14SK0uibENIo5H0O_Ma6KpP2rF1TbgEreu3R2Ukib0Vr9bJds2L-g?stp=s2048x1152&ccb=10-5&oh=00_AfBb33hgHaxhlRnrovVaeukq6ZdKdcY_6Meb-OI7UGSYzQ&oe=65B33919&_nc_sid=201bca\"\n",
    "\n",
    "# response = requests.get(url145_2)\n",
    "# img = Image.open(BytesIO(response.content))\n",
    "# display(img)\n",
    "\n",
    "# response10 = requests.get(url145_x)\n",
    "# img10 = Image.open(BytesIO(response10.content))\n",
    "\n",
    "# hashing = imagehash.average_hash(img)\n",
    "# hashing10 = imagehash.average_hash(img10)\n",
    "\n",
    "# print(hashing)\n",
    "# print(hashing10)\n",
    "# print(abs(hashing-hashing10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the model\n",
    "def build_siamese_network(input_shape):\n",
    "    left_input = layers.Input(shape=input_shape)\n",
    "    right_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Shared convolutional layers\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Encoding for both left and right inputs\n",
    "    encoded_left = model(left_input)\n",
    "    encoded_right = model(right_input)\n",
    "\n",
    "    # L1 distance layer between the two encoded representations\n",
    "    L1_layer = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "\n",
    "    # Add the distance layer to the network\n",
    "    L1_distance = L1_layer([encoded_left, encoded_right])\n",
    "\n",
    "    # Prediction layer\n",
    "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Create and compile the model\n",
    "    siamese_model = models.Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    siamese_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "# Function to load and preprocess images from URL\n",
    "def preprocess_image_from_url(image_url, target_size):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = tf.cast(img_array, tf.float32) / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "    return img_array\n",
    "\n",
    "# Example usage\n",
    "image1_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9cxUlW_AayC--U1c1tXtigRWBRepZES4Saq31GZPPfI_1kASse71rEi1aHOXJg7PgemaXitnPVZj7-Gu3DCgPYpqEpwxl8xyqfFQExorMuMYfThwfTk1o7hkNvHVDmUNNzmOxxMihFXrV1DWZbUQ?stp=s2048x1152&ccb=10-5&oh=00_AfABtAA7BtX7l74UEQ9ZLtj1PaNJfoLMuund6A1JT3mVIQ&oe=65B3421E&_nc_sid=201bca\"\n",
    "image2_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An_KYEqGfSXn0q4WRrqa5yKMBoBGH2ASguDbtT_RWVlbONyd3wa2GQn_D2n5o1hEzzh3HJjCCRpBnakmbUbOvOxrjKhCTQD0j14SK0uibENIo5H0O_Ma6KpP2rF1TbgEreu3R2Ukib0Vr9bJds2L-g?stp=s2048x1152&ccb=10-5&oh=00_AfBb33hgHaxhlRnrovVaeukq6ZdKdcY_6Meb-OI7UGSYzQ&oe=65B33919&_nc_sid=201bca\"\n",
    "image3_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "image4_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An-0tQBuUEa8IDZHhx03VckPyeF658vlfksT9BcLeybQ1GtmbFVQlD6O0y3oPx4ehZV5PFiFs63E9Y48CCTirCw1Be6vT2cLzsi-jKYLwOZ2fWm94_V1irwiyEWTsd03qRVvu3ugOoa72YF9V_cHDg?stp=s2048x1536&ccb=10-5&oh=00_AfDfkLUS9BEDGUfpOkuQQp_Jx-fLhpoUROJVJuaU0XfXHw&oe=65B32AC6&_nc_sid=201bca\"\n",
    "\n",
    "target_size = (128, 128)  # Adjust the target size as needed\n",
    "\n",
    "image1 = preprocess_image_from_url(image1_url, target_size)\n",
    "image2 = preprocess_image_from_url(image2_url, target_size)\n",
    "image3 = preprocess_image_from_url(image3_url, target_size)\n",
    "image4 = preprocess_image_from_url(image4_url, target_size)\n",
    "\n",
    "input_shape = image1.shape[1:]  # Shape of the preprocessed image\n",
    "\n",
    "# Combine pairs and labels into a list for training\n",
    "image_pairs = ([image1, image2], [image1, image4])\n",
    "\n",
    "# Separate the pairs into left and right inputs\n",
    "left_inputs = np.array(image_pairs)[:, 0, :, :, :]\n",
    "right_inputs = np.array(image_pairs)[:, 1, :, :, :]\n",
    "\n",
    "# Assuming you have labels indicating whether the pairs are similar or not\n",
    "labels = np.array([1, 0])  # 1 if similar, 0 if not\n",
    "\n",
    "# Reshape inputs to match the model's expectations\n",
    "# take note of this\n",
    "left_inputs = left_inputs.reshape((-1,) + left_inputs.shape[2:])\n",
    "right_inputs = right_inputs.reshape((-1,) + right_inputs.shape[2:])\n",
    "\n",
    "siamese_model = build_siamese_network(input_shape)\n",
    "siamese_model.summary()\n",
    "\n",
    "siamese_model.fit([left_inputs, right_inputs], labels, epochs=10, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for checking similarity score of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49899888"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have a trained Siamese model named 'siamese_model'\n",
    "\n",
    "# Load and preprocess two new images from URLs\n",
    "new_image1_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "new_image2_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "\n",
    "new_image1 = preprocess_image_from_url(new_image1_url, target_size)\n",
    "new_image2 = preprocess_image_from_url(new_image2_url, target_size)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = siamese_model.predict([new_image1, new_image2])\n",
    "\n",
    "# Interpret the predictions\n",
    "similarity_probability = predictions[0][0]\n",
    "\n",
    "similarity_probability\n",
    "# # Threshold for similarity (you can adjust this based on your needs)\n",
    "# similarity_threshold = 0.5\n",
    "\n",
    "# if similarity_probability >= similarity_threshold:\n",
    "#     print(\"The images are similar.\")\n",
    "# else:\n",
    "#     print(\"The images are dissimilar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m img_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m img_df\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\dbyon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "img_df = pd.read_csv(\"train.csv\")\n",
    "img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m column_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      7\u001b[0m column_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m     csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(file)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Iterate through each row and append the value in the specified column to the list\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "# make a list of each column, might be a code somewhere \n",
    "import csv \n",
    "\n",
    "csv_file_path = \"train.csv\"\n",
    "\n",
    "column_index = 2\n",
    "\n",
    "column_values = []\n",
    "\n",
    "with open(csv_file_path, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate through each row and append the value in the specified column to the list\n",
    "    for row in csv_reader:\n",
    "        if len(row) > column_index:  # Check if the column exists in the current row\n",
    "            column_values.append(row[column_index])\n",
    "\n",
    "print(column_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2957\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2957\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filtered_gdf_list \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_similar_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_geojson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mfilter_similar_images\u001b[1;34m(gdf_list)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gdf \u001b[38;5;129;01min\u001b[39;00m gdf_list:\n\u001b[0;32m     22\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthumb_2048_url\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace 'your_image_column' with the column containing image file paths\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     file_hash \u001b[38;5;241m=\u001b[39m \u001b[43mhash_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_hash \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hash_set:\n\u001b[0;32m     26\u001b[0m         hash_set\u001b[38;5;241m.\u001b[39madd(file_hash)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mhash_image\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_image\u001b[39m(file_path):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Compute a hash for an image file.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m imagehash\u001b[38;5;241m.\u001b[39maverage_hash(img)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2959\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2957\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n\u001b[1;32m-> 2959\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m())\n\u001b[0;32m   2960\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2962\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "filtered_gdf_list = filter_similar_images(gdf_list)\n",
    "\n",
    "output_dir = 'output_geojson'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "merged_gdf = gpd.GeoDataFrame(pd.concat(filtered_gdf_list, ignore_index=True), crs=filtered_gdf_list[0].crs)\n",
    "output_filename = 'output_geojson/merged_paranaque.geojson'\n",
    "merged_gdf.to_file(output_filename, driver='GeoJSON')\n",
    "print(f\"Saved {output_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
