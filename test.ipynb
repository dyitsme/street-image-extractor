{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing of Requirements (only run this if you haven't installed the necessary libraries found in requirements.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as  gpd\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "- convert all geojson files into 1 geodataframe \n",
    "- data cleaning\n",
    "- clip the inputs onto the map layer, similar to getting the intersection between the points and the map layer\n",
    "- return the output onto a file\n",
    "- write documentation in this notebook if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Paranaque Images and Saving a compiled GEOJSON file of the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the data of Paranque images\n",
    "paranaque_json_dir = 'data/paranaque'\n",
    "\n",
    "os.chdir(paranaque_json_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_list = []\n",
    "geojson_Count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    if file.endswith('.geojson'):\n",
    "        gdf = gpd.read_file(file)\n",
    "        geojson_Count += 1\n",
    "        \n",
    "        # filter out null json files\n",
    "        if '\"features\": []' not in gdf:\n",
    "            gdf = gpd.read_file(file)\n",
    "            gdf_list.append(gdf)\n",
    "            \n",
    "            \n",
    "        # gdf_list.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this if you want to view the compiled json\n",
    "\n",
    "# gdf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the compiled images into a GeoDataFrame in a GEOJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = 'output_geojson'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# merged_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=gdf_list[0].crs)\n",
    "# output_filename = 'output_geojson/merged_paranaque.geojson'\n",
    "# merged_gdf.to_file(output_filename, driver='GeoJSON')\n",
    "# print(f\"Saved {output_filename}\")\n",
    "\n",
    "#the file is found in a folder that can be viewed in the data/paranaque folder as \"output_geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "def hash_image(file_path):\n",
    "    \"\"\"\n",
    "    Compute a hash for an image file.\n",
    "    \"\"\"\n",
    "    with Image.open(file_path) as img:\n",
    "        return imagehash.average_hash(img)\n",
    "\n",
    "def filter_similar_images(gdf_list):\n",
    "    \"\"\"\n",
    "    Filter out similar images based on their hash.\n",
    "    \"\"\"\n",
    "    filtered_gdf_list = []\n",
    "    hash_set = set()\n",
    "\n",
    "    for gdf in gdf_list:\n",
    "        file_path = gdf['thumb_2048_url']  # Replace 'your_image_column' with the column containing image file paths\n",
    "        file_hash = hash_image(file_path)\n",
    "\n",
    "        if file_hash not in hash_set:\n",
    "            hash_set.add(file_hash)\n",
    "            filtered_gdf_list.append(gdf)\n",
    "\n",
    "    return filtered_gdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 128)                  7392320   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 128)                  0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    129       ['lambda[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7392449 (28.20 MB)\n",
      "Trainable params: 7392449 (28.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 1s 970ms/step - loss: 0.6816 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7886 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1535 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.2666e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3280e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.1646e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1609e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x197f83340a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the model\n",
    "def build_siamese_network(input_shape):\n",
    "    left_input = layers.Input(shape=input_shape)\n",
    "    right_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Shared convolutional layers\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Encoding for both left and right inputs\n",
    "    encoded_left = model(left_input)\n",
    "    encoded_right = model(right_input)\n",
    "\n",
    "    # L1 distance layer between the two encoded representations\n",
    "    L1_layer = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "\n",
    "    # Add the distance layer to the network\n",
    "    L1_distance = L1_layer([encoded_left, encoded_right])\n",
    "\n",
    "    # Prediction layer\n",
    "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Create and compile the model\n",
    "    siamese_model = models.Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    siamese_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "# Function to load and preprocess images from URL\n",
    "def preprocess_image_from_url(image_url, target_size):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = tf.cast(img_array, tf.float32) / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "    return img_array\n",
    "\n",
    "# Example usage\n",
    "image1_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9cxUlW_AayC--U1c1tXtigRWBRepZES4Saq31GZPPfI_1kASse71rEi1aHOXJg7PgemaXitnPVZj7-Gu3DCgPYpqEpwxl8xyqfFQExorMuMYfThwfTk1o7hkNvHVDmUNNzmOxxMihFXrV1DWZbUQ?stp=s2048x1152&ccb=10-5&oh=00_AfABtAA7BtX7l74UEQ9ZLtj1PaNJfoLMuund6A1JT3mVIQ&oe=65B3421E&_nc_sid=201bca\"\n",
    "image2_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An_KYEqGfSXn0q4WRrqa5yKMBoBGH2ASguDbtT_RWVlbONyd3wa2GQn_D2n5o1hEzzh3HJjCCRpBnakmbUbOvOxrjKhCTQD0j14SK0uibENIo5H0O_Ma6KpP2rF1TbgEreu3R2Ukib0Vr9bJds2L-g?stp=s2048x1152&ccb=10-5&oh=00_AfBb33hgHaxhlRnrovVaeukq6ZdKdcY_6Meb-OI7UGSYzQ&oe=65B33919&_nc_sid=201bca\"\n",
    "image3_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "image4_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An-0tQBuUEa8IDZHhx03VckPyeF658vlfksT9BcLeybQ1GtmbFVQlD6O0y3oPx4ehZV5PFiFs63E9Y48CCTirCw1Be6vT2cLzsi-jKYLwOZ2fWm94_V1irwiyEWTsd03qRVvu3ugOoa72YF9V_cHDg?stp=s2048x1536&ccb=10-5&oh=00_AfDfkLUS9BEDGUfpOkuQQp_Jx-fLhpoUROJVJuaU0XfXHw&oe=65B32AC6&_nc_sid=201bca\"\n",
    "\n",
    "target_size = (128, 128)  # Adjust the target size as needed\n",
    "\n",
    "image1 = preprocess_image_from_url(image1_url, target_size)\n",
    "image2 = preprocess_image_from_url(image2_url, target_size)\n",
    "image3 = preprocess_image_from_url(image3_url, target_size)\n",
    "image4 = preprocess_image_from_url(image4_url, target_size)\n",
    "\n",
    "input_shape = image1.shape[1:]  # Shape of the preprocessed image\n",
    "\n",
    "# Combine pairs and labels into a list for training\n",
    "image_pairs = ([image1, image2], [image1, image4])\n",
    "\n",
    "# Separate the pairs into left and right inputs\n",
    "left_inputs = np.array(image_pairs)[:, 0, :, :, :]\n",
    "right_inputs = np.array(image_pairs)[:, 1, :, :, :]\n",
    "\n",
    "# Assuming you have labels indicating whether the pairs are similar or not\n",
    "labels = np.array([1, 0])  # 1 if similar, 0 if not\n",
    "\n",
    "# Reshape inputs to match the model's expectations\n",
    "# take note of this\n",
    "left_inputs = left_inputs.reshape((-1,) + left_inputs.shape[2:])\n",
    "right_inputs = right_inputs.reshape((-1,) + right_inputs.shape[2:])\n",
    "\n",
    "siamese_model = build_siamese_network(input_shape)\n",
    "siamese_model.summary()\n",
    "\n",
    "siamese_model.fit([left_inputs, right_inputs], labels, epochs=10, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for checking similarity score of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5002883"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you have a trained Siamese model named 'siamese_model'\n",
    "\n",
    "# Load and preprocess two new images from URLs\n",
    "new_image1_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "new_image2_url = \"https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t6/An9WdAlvQB4b3vVt8c0cKC5EvIAPus33Q0BT3_N-7Q7L4EFNsE0RNbe58SZmbc6tPZyUszp_UHNydb-UHZgf-kcHnpzU2Nma4bURfjzUPSQixw4NPeUU5fqN6LktzzsIX9rYg7GAOg4f6yAVPCBbnA?stp=s2048x1152&ccb=10-5&oh=00_AfA7aberfS7qHOSy63gM3E_QZqwdRSk74GbNmagfB2O3YQ&oe=65B34BA0&_nc_sid=201bca\"\n",
    "\n",
    "new_image1 = preprocess_image_from_url(new_image1_url, target_size)\n",
    "new_image2 = preprocess_image_from_url(new_image2_url, target_size)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = siamese_model.predict([new_image1, new_image2])\n",
    "\n",
    "# Interpret the predictions\n",
    "similarity_probability = predictions[0][0]\n",
    "\n",
    "similarity_probability\n",
    "# # Threshold for similarity (you can adjust this based on your needs)\n",
    "# similarity_threshold = 0.5\n",
    "\n",
    "# if similarity_probability >= similarity_threshold:\n",
    "#     print(\"The images are similar.\")\n",
    "# else:\n",
    "#     print(\"The images are dissimilar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Sean\\\\Documents\\\\Python Notebooks\\\\street-image-extractor'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img1</th>\n",
       "      <th>img2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  img1  \\\n",
       "0    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "1    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "2    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "3    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "4    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "..                                                 ...   \n",
       "133  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "134  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "135  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "136  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "137  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "\n",
       "                                                  img2  label  \n",
       "0    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "1    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "2    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "3    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "4    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "..                                                 ...    ...  \n",
       "133  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      0  \n",
       "134  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      0  \n",
       "135  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "136  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "137  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "\n",
       "[138 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df = pd.read_csv(\"train.csv\")\n",
    "img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img1 = img_df['img1'].tolist()\n",
    "# img2 = img_df['img2'].tolist()\n",
    "\n",
    "# labels = img_df['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = [preprocess_image_from_url(row, target_size) for row in img_df['img1'].tolist()]\n",
    "img2 = [preprocess_image_from_url(row, target_size) for row in img_df['img2'].tolist()]\n",
    "\n",
    "labels = img_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (128, 128)  # Adjust the target size as needed\n",
    "\n",
    "input_shape = (138, 138, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 1, 1, 138, 138, 3) (138, 1, 1, 138, 138, 3) (138,)\n"
     ]
    }
   ],
   "source": [
    "# Resize images to the desired shape (138, 138, 3) and add padding if necessary\n",
    "img1_resized = [tf.image.resize_with_pad(image, 138, 138) for image in img1]\n",
    "img2_resized = [tf.image.resize_with_pad(image, 138, 138) for image in img2]\n",
    "\n",
    "# Convert lists of resized images to NumPy arrays\n",
    "img1 = np.array([np.expand_dims(image, axis=0) for image in img1_resized])\n",
    "img2 = np.array([np.expand_dims(image, axis=0) for image in img2_resized])\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(img1.shape, img2.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/69 [=======================>......] - ETA: 0s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m img1 \u001b[38;5;241m=\u001b[39m img1\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m img1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m      7\u001b[0m img2 \u001b[38;5;241m=\u001b[39m img2\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m img2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m---> 10\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:1819\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1817\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). This could be due to issues in input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline that resulted in an empty dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1824\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1825\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1826\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1828\u001b[0m     )\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[0;32m   1830\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). This could be due to issues in input pipeline that resulted in an empty dataset. Otherwise, please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "img1 = np.array([preprocess_image_from_url(row, target_size) for row in img_df['img1'].tolist()])\n",
    "img2 = np.array([preprocess_image_from_url(row, target_size) for row in img_df['img2'].tolist()])\n",
    "\n",
    "labels = np.array(img_df['label'].tolist())\n",
    "\n",
    "img1 = img1.reshape((-1,) + img1.shape[2:])\n",
    "img2 = img2.reshape((-1,) + img2.shape[2:])\n",
    "\n",
    "\n",
    "siamese_model.fit([img1, img2], labels, epochs=100, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img1 shape: (138, 128, 128, 3)\n",
      "img2 shape: (138, 128, 128, 3)\n",
      "labels shape: (138,)\n"
     ]
    }
   ],
   "source": [
    "# print(\"img1 shape:\", img1.shape)\n",
    "# print(\"img2 shape:\", img2.shape)\n",
    "# print(\"labels shape:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)        [(None, 138, 138, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)        [(None, 138, 138, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 128)                  8940608   ['input_7[0][0]',             \n",
      "                                                                     'input_8[0][0]']             \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)           (None, 128)                  0         ['sequential_3[0][0]',        \n",
      "                                                                     'sequential_3[1][0]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    129       ['lambda_3[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8940737 (34.11 MB)\n",
      "Trainable params: 8940737 (34.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_3\" is incompatible with the layer: expected shape=(None, 138, 138, 3), found shape=(2, 1, 1, 138, 138, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file06xo2iqf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_3\" is incompatible with the layer: expected shape=(None, 138, 138, 3), found shape=(2, 1, 1, 138, 138, 3)\n"
     ]
    }
   ],
   "source": [
    "siamese_model = build_siamese_network(input_shape)\n",
    "siamese_model.summary()\n",
    "\n",
    "# Train the model\n",
    "siamese_model.fit([img1, img2], labels, epochs=100, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in img1:\n",
    "#     # image = preprocess_image_from_url(row, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in img2:\n",
    "#     image = preprocess_image_from_url(row, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)       [(None, 138, 138, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)       [(None, 138, 138, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)  (None, 128)                  8940608   ['input_25[0][0]',            \n",
      "                                                                     'input_26[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_12 (Lambda)          (None, 128)                  0         ['sequential_12[0][0]',       \n",
      "                                                                     'sequential_12[1][0]']       \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 1)                    129       ['lambda_12[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8940737 (34.11 MB)\n",
      "Trainable params: 8940737 (34.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_12\" is incompatible with the layer: expected shape=(None, 138, 138, 3), found shape=(2, 1, 128, 128, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m siamese_model \u001b[38;5;241m=\u001b[39m build_siamese_network(input_shape)\n\u001b[0;32m      9\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 11\u001b[0m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filept3psh10.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_12\" is incompatible with the layer: expected shape=(None, 138, 138, 3), found shape=(2, 1, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Convert lists to NumPy arrays\n",
    "# img1 = np.array(img1)\n",
    "# img2 = np.array(img2)\n",
    "\n",
    "# # Assuming 'labels' is also a list, convert it to a NumPy array\n",
    "# labels = np.array(labels)\n",
    "\n",
    "# siamese_model = build_siamese_network(input_shape)\n",
    "# siamese_model.summary()\n",
    "\n",
    "# siamese_model.fit([img1, img2], labels, epochs=100, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138,) (138,) (138,)\n"
     ]
    }
   ],
   "source": [
    "# print(img1.shape, img2.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2957\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2957\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m filtered_gdf_list \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_similar_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgdf_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_geojson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mfilter_similar_images\u001b[1;34m(gdf_list)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gdf \u001b[38;5;129;01min\u001b[39;00m gdf_list:\n\u001b[0;32m     22\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthumb_2048_url\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace 'your_image_column' with the column containing image file paths\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     file_hash \u001b[38;5;241m=\u001b[39m \u001b[43mhash_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_hash \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m hash_set:\n\u001b[0;32m     26\u001b[0m         hash_set\u001b[38;5;241m.\u001b[39madd(file_hash)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mhash_image\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_image\u001b[39m(file_path):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Compute a hash for an image file.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m imagehash\u001b[38;5;241m.\u001b[39maverage_hash(img)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py:2959\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2957\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation):\n\u001b[1;32m-> 2959\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m())\n\u001b[0;32m   2960\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2962\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# filtered_gdf_list = filter_similar_images(gdf_list)\n",
    "\n",
    "# output_dir = 'output_geojson'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# merged_gdf = gpd.GeoDataFrame(pd.concat(filtered_gdf_list, ignore_index=True), crs=filtered_gdf_list[0].crs)\n",
    "# output_filename = 'output_geojson/merged_paranaque.geojson'\n",
    "# merged_gdf.to_file(output_filename, driver='GeoJSON')\n",
    "# print(f\"Saved {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW MODEL ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sean\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Function to create the base convolutional neural network (CNN) for image embedding\n",
    "def create_base_network(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(input_layer)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    model = Model(inputs=input_layer, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Function to calculate the Euclidean distance between two image embeddings\n",
    "def euclidean_distance(vectors):\n",
    "    vector1, vector2 = vectors\n",
    "    sum_squared = K.sum(K.square(vector1 - vector2), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_squared, K.epsilon()))\n",
    "\n",
    "# Function to create the Siamese network\n",
    "def create_siamese_network(input_shape):\n",
    "    input_image1 = Input(shape=input_shape)\n",
    "    input_image2 = Input(shape=input_shape)\n",
    "\n",
    "    base_network = create_base_network(input_shape)\n",
    "\n",
    "    # Getting the encoded representation of the input images\n",
    "    encoded_image1 = base_network(input_image1)\n",
    "    encoded_image2 = base_network(input_image2)\n",
    "\n",
    "    # Calculating the distance between the encoded representations\n",
    "    distance = Lambda(euclidean_distance)([encoded_image1, encoded_image2])\n",
    "\n",
    "    # Creating the Siamese model\n",
    "    siamese_model = Model(inputs=[input_image1, input_image2], outputs=distance)\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img1</th>\n",
       "      <th>img2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  img1  \\\n",
       "0    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "1    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "2    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "3    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "4    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "..                                                 ...   \n",
       "133  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "134  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "135  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "136  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "137  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...   \n",
       "\n",
       "                                                  img2  label  \n",
       "0    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "1    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "2    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "3    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "4    https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "..                                                 ...    ...  \n",
       "133  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      0  \n",
       "134  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      0  \n",
       "135  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "136  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "137  https://scontent.fmnl30-2.fna.fbcdn.net/m1/v/t...      1  \n",
       "\n",
       "[138 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df = pd.read_csv(\"train.csv\")\n",
    "img_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from io import BytesIO\n",
    "\n",
    "# Define the model\n",
    "def build_siamese_network(input_shape):\n",
    "    left_input = layers.Input(shape=input_shape)\n",
    "    right_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Shared convolutional layers\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Encoding for both left and right inputs\n",
    "    encoded_left = model(left_input)\n",
    "    encoded_right = model(right_input)\n",
    "\n",
    "    # L1 distance layer between the two encoded representations\n",
    "    L1_layer = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))\n",
    "\n",
    "    # Add the distance layer to the network\n",
    "    L1_distance = L1_layer([encoded_left, encoded_right])\n",
    "\n",
    "    # Prediction layer\n",
    "    prediction = layers.Dense(1, activation='sigmoid')(L1_distance)\n",
    "\n",
    "    # Create and compile the model\n",
    "    siamese_model = models.Model(inputs=[left_input, right_input], outputs=prediction)\n",
    "    siamese_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "def preprocess_image_from_url(image_url, target_size):\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize(target_size)\n",
    "    img_array = np.array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "    img_array = tf.cast(img_array, tf.float32) / 255.0  # Normalize pixel values to be between 0 and 1\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (205, 205, 3)  # Adjust input shape based on your images\n",
    "target_size = (205, 205) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img1 = np.array([preprocess_image_from_url(row, target_size) for row in img_df['img1'].tolist()])\n",
    "img2 = np.array([preprocess_image_from_url(row, target_size) for row in img_df['img2'].tolist()])\n",
    "\n",
    "labels = np.array(img_df['label'].tolist())\n",
    "\n",
    "img1 = img1.reshape((-1,) + img1.shape[2:])\n",
    "img2 = img2.reshape((-1,) + img2.shape[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)       [(None, 205, 205, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)       [(None, 205, 205, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 128)                  1968851   ['input_16[0][0]',            \n",
      "                                                          2          'input_17[0][0]']            \n",
      "                                                                                                  \n",
      " lambda_6 (Lambda)           (None, 128)                  0         ['sequential_3[0][0]',        \n",
      "                                                                     'sequential_3[1][0]']        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 1)                    129       ['lambda_6[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19688641 (75.11 MB)\n",
      "Trainable params: 19688641 (75.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 3s 419ms/step - loss: 0.3536 - accuracy: 0.8406\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.1537 - accuracy: 0.9275\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.1048 - accuracy: 0.9565\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.0504 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 3s 508ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 2s 402ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 416ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 418ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 2s 421ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 2s 407ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 2s 400ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 2s 379ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 2s 387ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 2s 400ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 2s 400ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 2s 382ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 2s 406ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 383ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 2s 397ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 2s 377ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 2s 393ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 2s 390ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 2s 388ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 2s 400ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 2s 385ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 2s 384ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 2s 395ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 2s 403ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 2s 391ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 2s 389ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 2s 394ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 2s 381ms/step - loss: 0.0093 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1977f481c10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "siamese_model = build_siamese_network(input_shape)\n",
    "\n",
    "# siamese_model = create_siamese_network(input_shape)\n",
    "siamese_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "siamese_model.summary()\n",
    "\n",
    "\n",
    "# Training the Siamese network with your image pairs and similarity scores\n",
    "# X_train_image1, X_train_image2 are the image pairs, and y_train is the similarity score (1 or 0)\n",
    "siamese_model.fit([img1, img2], labels, epochs=100, batch_size=32)\n",
    "\n",
    "# After training, you can use the model to predict the similarity between two new images\n",
    "# similarity_score = siamese_model.predict([image1, image2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "0.041213818\n",
      "The images are similar.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a trained Siamese model named 'siamese_model'\n",
    "\n",
    "# Load and preprocess two new images from URLs\n",
    "new_image1_url = \"https://scontent.fmnl17-5.fna.fbcdn.net/m1/v/t6/An_lvFyyeHWaA-AbolXOaEV15UuxBfj47oSxTDm4DphxMJiTOmQmiprhxavhYwQj4HFFQMKyDm5xtaCFAwFP_kcu1v9CKYZpWkESkmfj_mpPIu5zFKy5vHPHhzfY1wfqWzexos_Gctg_mUR1pbdG2MM?stp=s2048x1152&ccb=10-5&oh=00_AfDeBrhFbPRRxiHa37hIMGrfiSke2tCkuDkeYMWwTEEUAg&oe=65CEC08E&_nc_sid=201bca\"\n",
    "new_image2_url = \"https://scontent.fmnl17-5.fna.fbcdn.net/m1/v/t6/An8l2r73oTUMYSbZkOogu4jRHbAFQOxAJGM7gtKWVNpPLnOkYMG8RJXmhK4mkzhz9pn4HjwMfM01HDgFns5bmXyc_vQ0sAI9IPMaO8-Q9j3DwB9YR0yZDtHGfoMlEin4nMXyuDKzQxAijP1KyzWNGM4?stp=s2048x1152&ccb=10-5&oh=00_AfACjdBjotxJCVGcOvr8rlfyVfkGg4Qw5KHZpiUZcriDFQ&oe=65CEA448&_nc_sid=201bca\"\n",
    "\n",
    "new_image1 = preprocess_image_from_url(new_image1_url, target_size)\n",
    "new_image2 = preprocess_image_from_url(new_image2_url, target_size)\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = siamese_model.predict([new_image1, new_image2])\n",
    "\n",
    "# Interpret the predictions\n",
    "similarity_probability = predictions[0][0]\n",
    "\n",
    "print(similarity_probability)\n",
    "\n",
    "# # Threshold for similarity (you can adjust this based on your needs)\n",
    "similarity_threshold = 0.001\n",
    "\n",
    "if similarity_probability >= similarity_threshold:\n",
    "    print(\"The images are similar.\")\n",
    "else:\n",
    "    print(\"The images are dissimilar.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
